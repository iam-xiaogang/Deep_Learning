# Deep_Learning
study deep learning code
# **LeNet**

## 概述：
- **LeNet** 是最早的卷积神经网络之一，主要用于手写数字识别（如 MNIST 数据集）。
- 包含两个卷积层和几个全连接层。

## **优点：**
- 简单且易于实现。
- 在小数据集上表现良好，特别是手写数字识别。
- 计算效率较高，轻量化。

## **缺点：**
- 网络架构较浅，对于更复杂的任务（如大规模图像分类）效果较差。
- 无法很好地泛化到更复杂的任务上，相比现代的深度网络表现不够优秀。

---

# **AlexNet**

## 概述：
- **AlexNet** 在 2012 年的 ImageNet 比赛中获得冠军，显著提升了深度学习模型在大规模图像分类上的表现。
- 由五个卷积层和三个全连接层组成。

## **优点：**
- 在大规模数据集（如 ImageNet）上表现出色。
- 引入了 **ReLU 激活函数**、**GPU 加速** 和 **Dropout** 技术，有效防止过拟合。

## **缺点：**
- 网络较大，计算开销较高。
- 深度较深，容易出现梯度消失或过拟合的问题，尤其在没有适当处理的情况下。

---

# **VGG**

## 概述：
- **VGG** 是一个非常深的卷积神经网络，因其简单性和深度（最多 19 层）而著名。
- 使用小的 3x3 卷积核，目的是通过堆叠更多的层来增加感受野。

## **优点：**
- 简单且结构统一。
- 在多种图像识别任务中表现良好。
- 使用小的卷积核使得网络易于修改和实验。

## **缺点：**
- 参数量极大，容易导致过拟合，并且需要大量计算资源。
- 训练速度慢，因为网络较深。

---

# **GoogLeNet（Inception）**

## 概述：
- **GoogLeNet** 引入了 **Inception 模块**，每一层都有多种滤波器大小，可以从多个尺度提取特征。
- 该架构较深，达到 22 层，但由于其高效的设计，参数较少。

## **优点：**
- 相比于之前的模型，使用较少的参数，效率更高。
- 引入了 **Inception 模块**，允许网络在多个尺度和深度上学习特征。
- 适用于高分辨率图像。

## **缺点：**
- **Inception 模块** 的复杂结构使得模型较难理解和修改。
- 由于深度和架构复杂性，调参和优化较为困难。

---

# **ResNet**

## 概述：
- **ResNet**（残差网络）引入了 **跳跃连接**（残差连接）的概念，这使得梯度能够更容易地传播到更深的网络。
- ResNet 是目前最深的网络之一，最多可达到 152 层。

## **优点：**
- **跳跃连接**（残差连接）有效缓解了梯度消失问题，使得非常深的网络能够训练。
- 可以有效训练非常深的网络而不容易过拟合。
- 在多个任务上取得了 **最先进的性能**（例如 ImageNet）。

## **缺点：**
- 非常深的网络计算开销较大。
- 网络复杂且参数较多，可能使得实现和调优变得更加困难，尤其是在较小数据集上的应用。
